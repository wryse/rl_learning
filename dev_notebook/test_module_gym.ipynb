{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import simple_nn.activation_function as actv_func\n",
    "import simple_nn.gd_updater as gd_upd\n",
    "import simple_nn.nn_model as nn\n",
    "import simple_nn.nn_layer as nn_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common op for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_tgt_values(batch_action,\n",
    "                            batch_q_values, \n",
    "                            batch_next_q_values, \n",
    "                            batch_reward, \n",
    "                            batch_end):\n",
    "    tgt_rewards = batch_reward + discount * np.max(batch_next_q_values, axis=1) * (1-batch_end)\n",
    "    tgt_values = batch_q_values.copy()\n",
    "    tgt_values[np.arange(len(tgt_values)),batch_action] = tgt_rewards\n",
    "    \n",
    "    return tgt_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary expand categorical type to preprocess states\n",
    "def binary_expand(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    res = np.zeros(idx.shape)\n",
    "    res[n==idx] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_expand_mod(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    return np.identity(len(idx))[np.where(n==idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess state information into input parameters\n",
    "preprocess_state = binary_expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning agent - online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 15000\n",
    "learning_rate = 0.02\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.001\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.NNModel(env.observation_space.n, gd_upd.GDUpdaterNormal(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_layer(nn_l.FCLayer(20, True, actv_func.ActivationNone()))\n",
    "m.add_layer(nn_l.FCLayer(env.action_space.n, True, actv_func.ActivationNone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 48.721 steps are used, avg total reward is 0.005.\n",
      "In episode 2000, avg 74.534 steps are used, avg total reward is 0.018.\n",
      "In episode 3000, avg 90.089 steps are used, avg total reward is 0.065.\n",
      "In episode 4000, avg 71.354 steps are used, avg total reward is 0.157.\n",
      "In episode 5000, avg 76.702 steps are used, avg total reward is 0.212.\n",
      "In episode 6000, avg 75.56 steps are used, avg total reward is 0.221.\n",
      "In episode 7000, avg 76.437 steps are used, avg total reward is 0.253.\n",
      "In episode 8000, avg 79.83 steps are used, avg total reward is 0.419.\n",
      "In episode 9000, avg 94.433 steps are used, avg total reward is 0.754.\n",
      "In episode 10000, avg 94.791 steps are used, avg total reward is 0.743.\n",
      "In episode 11000, avg 95.926 steps are used, avg total reward is 0.746.\n",
      "In episode 12000, avg 94.365 steps are used, avg total reward is 0.748.\n",
      "In episode 13000, avg 95.189 steps are used, avg total reward is 0.751.\n",
      "In episode 14000, avg 96.805 steps are used, avg total reward is 0.739.\n",
      "In episode 15000, avg 95.428 steps are used, avg total reward is 0.762.\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values = m.model_forward(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # calculate error for back propagtion with Bellman equation\n",
    "        tgt_q_values = q_values.copy()\n",
    "        next_q_values = m.predict(new_state)\n",
    "        tgt_q_values[action] = reward + discount * np.max(next_q_values) * (not end)\n",
    "                \n",
    "        # update model with predicted q values\n",
    "        m.update_model(q_values, tgt_q_values)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFF\u001b[41mH\u001b[0mFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 2, 2, 2, 0],\n",
       "       [3, 3, 3, 3, 3, 2, 2, 0],\n",
       "       [3, 3, 0, 0, 2, 3, 2, 0],\n",
       "       [0, 3, 0, 1, 0, 0, 2, 2],\n",
       "       [0, 3, 3, 1, 2, 3, 3, 2],\n",
       "       [0, 0, 1, 1, 3, 1, 1, 2],\n",
       "       [0, 0, 3, 3, 1, 0, 2, 2],\n",
       "       [3, 3, 0, 0, 0, 3, 2, 0]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = m.predict(np.identity(64))\n",
    "np.argmax(q_values, axis=1).reshape(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.7005\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for _ in range(test_episode):\n",
    "    end = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    while not end:\n",
    "        q_values = m.predict(state)\n",
    "        action = np.argmax(q_values)\n",
    "        state_no, reward, end, _ = env.step(action)\n",
    "        state = preprocess_state(state_no)\n",
    "        total_reward += reward\n",
    "print('average reward: {}'.format(total_reward/test_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning agent - experience replay with mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10000\n",
    "learning_rate = 0.1\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.01\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "max_buf_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.NNModel(env.observation_space.n, gd_upd.GDUpdaterNormal(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_layer(nn_l.FCLayer(20, True, actv_func.ActivationNone()))\n",
    "m.add_layer(nn_l.FCLayer(env.action_space.n, True, actv_func.ActivationNone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 89.163 steps are used, avg total reward is 0.162.\n",
      "In episode 2000, avg 96.15 steps are used, avg total reward is 0.385.\n",
      "In episode 3000, avg 98.227 steps are used, avg total reward is 0.644.\n",
      "In episode 4000, avg 95.054 steps are used, avg total reward is 0.841.\n",
      "In episode 5000, avg 102.165 steps are used, avg total reward is 0.801.\n",
      "In episode 6000, avg 93.573 steps are used, avg total reward is 0.812.\n",
      "In episode 7000, avg 94.897 steps are used, avg total reward is 0.816.\n",
      "In episode 8000, avg 92.835 steps are used, avg total reward is 0.812.\n",
      "In episode 9000, avg 98.056 steps are used, avg total reward is 0.777.\n",
      "In episode 10000, avg 96.09 steps are used, avg total reward is 0.783.\n",
      "Wall time: 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "\n",
    "experience_buf = deque(maxlen=max_buf_size)\n",
    "\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values = m.predict(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # record to the experience buf\n",
    "        experience_buf.append((state, action, reward, new_state, end))\n",
    "        \n",
    "        # sample and mini batch training\n",
    "        if len(experience_buf) > batch_size:\n",
    "            batch_samples_idx = np.random.choice(np.arange(len(experience_buf)), \n",
    "                                                 size=batch_size, replace=False)\n",
    "            batch_samples = [experience_buf[i] for i in batch_samples_idx]\n",
    "            batch_states = np.array([sample[0] for sample in batch_samples])\n",
    "            batch_action = np.array([sample[1] for sample in batch_samples])\n",
    "            batch_reward = np.array([sample[2] for sample in batch_samples])\n",
    "            batch_new_states = np.array([sample[3] for sample in batch_samples])\n",
    "            batch_end = np.array([sample[4] for sample in batch_samples])\n",
    "            batch_q_values = m.model_forward(batch_states)\n",
    "            batch_next_q_values = m.predict(batch_new_states)\n",
    "\n",
    "            # calculate error for back propagtion with Bellman equation\n",
    "            batch_tgt_q_values = calculate_batch_tgt_values(batch_action,\n",
    "                                                            batch_q_values, \n",
    "                                                            batch_next_q_values, \n",
    "                                                            batch_reward, \n",
    "                                                            batch_end)\n",
    "\n",
    "            # update model with predicted q values\n",
    "            m.update_model(batch_q_values, batch_tgt_q_values)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [3, 2, 3, 3, 3, 2, 2, 2],\n",
       "       [3, 3, 0, 3, 2, 3, 2, 2],\n",
       "       [3, 3, 3, 3, 0, 3, 2, 2],\n",
       "       [3, 3, 3, 0, 2, 1, 3, 2],\n",
       "       [0, 3, 1, 1, 3, 0, 2, 2],\n",
       "       [1, 0, 2, 3, 3, 0, 2, 2],\n",
       "       [3, 3, 3, 2, 2, 1, 2, 0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = m.predict(np.identity(64))\n",
    "np.argmax(q_values, axis=1).reshape(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.848\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for _ in range(test_episode):\n",
    "    end = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    while not end:\n",
    "        q_values = m.predict(state)\n",
    "        action = np.argmax(q_values)\n",
    "        state_no, reward, end, _ = env.step(action)\n",
    "        state = preprocess_state(state_no)\n",
    "        total_reward += reward\n",
    "print('average reward: {}'.format(total_reward/test_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
