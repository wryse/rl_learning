{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise', under='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deep Q model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "def model_init(input_size, model_define):\n",
    "    model.clear()\n",
    "    prev_node_count = input_size\n",
    "    for node_count, activation_func, activation_derivative in model_define:\n",
    "        model.append([np.random.randn(prev_node_count, node_count)/np.sqrt(prev_node_count),\n",
    "                      np.random.randn(node_count)/np.sqrt(prev_node_count),\n",
    "                      activation_func,\n",
    "                      activation_derivative])\n",
    "        prev_node_count = node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass, calculate predict value with current model\n",
    "def model_forward(state):\n",
    "    cur_res = state\n",
    "    hidden_layer_input_buf = []\n",
    "    for layer_weight, inter_weight, activation_func, _ in model:\n",
    "        hidden_layer_input_buf.append(cur_res)\n",
    "        cur_res = np.dot(cur_res, layer_weight)\n",
    "        cur_res += inter_weight\n",
    "        if activation_func:\n",
    "            cur_res = activation_func(cur_res)\n",
    "    return cur_res, hidden_layer_input_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch back propagation to update model\n",
    "def back_propagation(errs, hidden_layer_input_buf):\n",
    "    grads = []\n",
    "    delta = np.atleast_2d(errs)\n",
    "    \n",
    "    # TODO: bug here, layers mismatched during calculating delta with layer activation derivative\n",
    "    for layer_out, (layer_weight, inter_weight, _, activation_derivative) \\\n",
    "        in zip(reversed(hidden_layer_input_buf), reversed(model)):\n",
    "        grads.append((np.dot(np.atleast_2d(layer_out).T, delta)/delta.shape[0],\n",
    "                      delta.mean(axis=0)))\n",
    "        delta = np.dot(delta, layer_weight.T)\n",
    "        if activation_derivative:\n",
    "            delta = delta * activation_derivative(layer_out)\n",
    "    grads.reverse()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update model with gradients\n",
    "def update_model(grads):\n",
    "    for i in range(len(model)):\n",
    "        model[i][0] = model[i][0] + learning_rate * grads[i][0]\n",
    "        model[i][1] = model[i][1] + learning_rate * grads[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common op for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_td_errs(batch_action,\n",
    "                            batch_q_values, \n",
    "                            batch_next_q_values, \n",
    "                            batch_reward, \n",
    "                            batch_end):\n",
    "    tgt_rewards = batch_reward + discount * np.max(batch_next_q_values, axis=1) * (1-batch_end)\n",
    "    td_errs = np.zeros(batch_q_values.shape)\n",
    "    td_errs[np.arange(len(td_errs)),batch_action] = \\\n",
    "        tgt_rewards - batch_q_values[np.arange(len(batch_q_values)),batch_action]\n",
    "    \n",
    "    return td_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake-v0'\n",
    "# env_name = 'FrozenLake8x8-v0'\n",
    "# env_name = 'FrozenLakeNotSlippery-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary expand categorical type to preprocess states\n",
    "def binary_expand(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    res = np.zeros(idx.shape)\n",
    "    res[n==idx] = 1\n",
    "    return res\n",
    "\n",
    "preprocess_func = binary_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_expand_mod(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    return np.identity(len(idx))[np.where(n==idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: sigmoid\n",
    "def sigmoid(v):\n",
    "    return 1.0 / (1.0 + np.exp(-v.clip(max=500,min=-500)))\n",
    "\n",
    "def sigmoid_derivative(sig_v):\n",
    "    return sig_v * (1 - sig_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active function: ReLU\n",
    "def relu(v):\n",
    "    return v.clip(min=0)\n",
    "\n",
    "def relu_derivative(v):\n",
    "    return np.where(v>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn model define by layers:\n",
    "# (node_count, active_function)\n",
    "model_define = [\n",
    "#     (20, relu, relu_derivative),\n",
    "#     (env.action_space.n, sigmoid, sigmoid_derivative),\n",
    "    (20, None, None),\n",
    "    (env.action_space.n, None, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess state information into input parameters\n",
    "preprocess_state = preprocess_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning agent - online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30000\n",
    "learning_rate = 0.02\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.001\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init(env.observation_space.n, model_define)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 11.014 steps are used, avg total reward is 0.037.\n",
      "In episode 2000, avg 21.638 steps are used, avg total reward is 0.18.\n",
      "In episode 3000, avg 32.584 steps are used, avg total reward is 0.427.\n",
      "In episode 4000, avg 39.059 steps are used, avg total reward is 0.532.\n",
      "In episode 5000, avg 42.098 steps are used, avg total reward is 0.611.\n",
      "In episode 6000, avg 43.445 steps are used, avg total reward is 0.673.\n",
      "In episode 7000, avg 44.35 steps are used, avg total reward is 0.638.\n",
      "In episode 8000, avg 43.389 steps are used, avg total reward is 0.674.\n",
      "In episode 9000, avg 43.227 steps are used, avg total reward is 0.688.\n",
      "In episode 10000, avg 41.921 steps are used, avg total reward is 0.688.\n",
      "In episode 11000, avg 43.802 steps are used, avg total reward is 0.66.\n",
      "In episode 12000, avg 42.588 steps are used, avg total reward is 0.717.\n",
      "In episode 13000, avg 42.462 steps are used, avg total reward is 0.682.\n",
      "In episode 14000, avg 42.627 steps are used, avg total reward is 0.71.\n",
      "In episode 15000, avg 42.659 steps are used, avg total reward is 0.683.\n",
      "In episode 16000, avg 43.398 steps are used, avg total reward is 0.696.\n",
      "In episode 17000, avg 42.608 steps are used, avg total reward is 0.68.\n",
      "In episode 18000, avg 42.684 steps are used, avg total reward is 0.694.\n",
      "In episode 19000, avg 44.235 steps are used, avg total reward is 0.689.\n",
      "In episode 20000, avg 42.251 steps are used, avg total reward is 0.688.\n",
      "In episode 21000, avg 41.884 steps are used, avg total reward is 0.697.\n",
      "In episode 22000, avg 42.972 steps are used, avg total reward is 0.682.\n",
      "In episode 23000, avg 43.445 steps are used, avg total reward is 0.661.\n",
      "In episode 24000, avg 43.851 steps are used, avg total reward is 0.693.\n",
      "In episode 25000, avg 43.04 steps are used, avg total reward is 0.688.\n",
      "In episode 26000, avg 42.893 steps are used, avg total reward is 0.683.\n",
      "In episode 27000, avg 42.995 steps are used, avg total reward is 0.674.\n",
      "In episode 28000, avg 40.853 steps are used, avg total reward is 0.678.\n",
      "In episode 29000, avg 41.429 steps are used, avg total reward is 0.702.\n",
      "In episode 30000, avg 44.262 steps are used, avg total reward is 0.691.\n"
     ]
    }
   ],
   "source": [
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values, hidden_layer_input_buf = model_forward(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # calculate error for back propagtion with Bellman equation\n",
    "        td_err = np.zeros_like(q_values)\n",
    "        next_q_values, _ = model_forward(new_state)\n",
    "        td_err[action] = (reward + discount * np.max(next_q_values) * (not end) - q_values[action])\n",
    "        \n",
    "        # back propagation with td error\n",
    "        grads = back_propagation(td_err, hidden_layer_input_buf)\n",
    "        \n",
    "        # update model with gradients\n",
    "        update_model(grads)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 3],\n",
       "       [0, 2, 0, 3],\n",
       "       [3, 1, 0, 1],\n",
       "       [3, 2, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values, _ = model_forward(np.identity(16))\n",
    "np.argmax(q_values, axis=1).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.7376\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for _ in range(test_episode):\n",
    "    end = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    while not end:\n",
    "        q_values, _ = model_forward(state)\n",
    "        action = np.argmax(q_values)\n",
    "        state_no, reward, end, _ = env.step(action)\n",
    "        state = preprocess_state(state_no)\n",
    "        total_reward += reward\n",
    "print('average reward: {}'.format(total_reward/test_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning agent - experience replay with mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10000\n",
    "learning_rate = 0.1\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.01\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "max_buf_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init(env.observation_space.n, model_define)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 35.227 steps are used, avg total reward is 0.509.\n",
      "In episode 2000, avg 41.4 steps are used, avg total reward is 0.681.\n",
      "In episode 3000, avg 43.389 steps are used, avg total reward is 0.667.\n",
      "In episode 4000, avg 43.945 steps are used, avg total reward is 0.685.\n",
      "In episode 5000, avg 44.573 steps are used, avg total reward is 0.669.\n",
      "In episode 6000, avg 41.986 steps are used, avg total reward is 0.667.\n",
      "In episode 7000, avg 42.284 steps are used, avg total reward is 0.699.\n",
      "In episode 8000, avg 43.899 steps are used, avg total reward is 0.657.\n",
      "In episode 9000, avg 43.636 steps are used, avg total reward is 0.713.\n",
      "In episode 10000, avg 41.808 steps are used, avg total reward is 0.656.\n"
     ]
    }
   ],
   "source": [
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "\n",
    "experience_buf = deque(maxlen=max_buf_size)\n",
    "\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values, _ = model_forward(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # record to the experience buf\n",
    "        experience_buf.append((state, action, reward, new_state, end))\n",
    "        \n",
    "        # sample and mini batch training\n",
    "        if len(experience_buf) > batch_size:\n",
    "            batch_samples_idx = np.random.choice(np.arange(len(experience_buf)), \n",
    "                                                 size=batch_size, replace=False)\n",
    "            batch_samples = [experience_buf[i] for i in batch_samples_idx]\n",
    "            batch_states = np.array([sample[0] for sample in batch_samples])\n",
    "            batch_action = np.array([sample[1] for sample in batch_samples])\n",
    "            batch_reward = np.array([sample[2] for sample in batch_samples])\n",
    "            batch_new_states = np.array([sample[3] for sample in batch_samples])\n",
    "            batch_end = np.array([sample[4] for sample in batch_samples])\n",
    "            batch_q_values, batch_hidden_layers = model_forward(batch_states)\n",
    "            batch_next_q_values, _ = model_forward(batch_new_states)\n",
    "\n",
    "            # calculate error for back propagtion with Bellman equation\n",
    "            batch_td_err = calculate_batch_td_errs(batch_action,\n",
    "                                                   batch_q_values, \n",
    "                                                   batch_next_q_values, \n",
    "                                                   batch_reward, \n",
    "                                                   batch_end)\n",
    "\n",
    "            # back propagation with td error\n",
    "            grads = back_propagation(batch_td_err, batch_hidden_layers)\n",
    "\n",
    "            # update model with gradients\n",
    "            update_model(grads)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 3],\n",
       "       [0, 0, 2, 2],\n",
       "       [3, 1, 0, 3],\n",
       "       [0, 2, 1, 2]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values, _ = model_forward(np.identity(16))\n",
    "np.argmax(q_values, axis=1).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.7393\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for _ in range(test_episode):\n",
    "    end = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    while not end:\n",
    "        q_values, _ = model_forward(state)\n",
    "        action = np.argmax(q_values)\n",
    "        state_no, reward, end, _ = env.step(action)\n",
    "        state = preprocess_state(state_no)\n",
    "        total_reward += reward\n",
    "print('average reward: {}'.format(total_reward/test_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
