{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise', under='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(abc.ABC):\n",
    "    \"\"\"Base class for activation functions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Init function. Nothing needs to do in the base class.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Boolean: If derivative of the activation function can be calculated\n",
    "        using activated value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def apply(self, v):\n",
    "        \"\"\"Apply activation function on the data.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            Activated values\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            Derivative wrt input of activation function\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationNone(ActivationFunction):\n",
    "    \"\"\"For not using an activation function\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Init function. Do nothing.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Arbitrary. Not used.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def apply(self, v):\n",
    "        \"\"\"Return input as output as no activation needs to be applied.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            The same as input v\n",
    "        \"\"\"\n",
    "        return v\n",
    "    \n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            Always 1\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSigmoid(ActivationFunction):\n",
    "    \"\"\"Sigmoid function. Data out of bounds will be clipped.\"\"\"\n",
    "    \n",
    "    def __init__(self, x_upper_bound=None, x_lower_bound=None):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Args:\n",
    "            x_upper_bound (int): Upper bound of sigmoid input\n",
    "            x_lower_bound (int): Lower bound of sigmoid input\n",
    "        \"\"\"\n",
    "        self.x_upper_bound = x_upper_bound\n",
    "        self.x_lower_bound = x_lower_bound\n",
    "    \n",
    "    @property\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Use sigmoid result to calculate derivative.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def apply(self, v):\n",
    "        \"\"\"Apply sigmoid function to data.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            1 / (1 + exp(v)). v will be clipped if bounds are set.\n",
    "        \"\"\"\n",
    "        if self.x_upper_bound or self.x_upper_bound:\n",
    "            return 1.0 / (1.0 + np.exp(-v.clip(max=self.x_upper_bound, min=self.x_lower_bound)))\n",
    "        return 1.0 / (1.0 + np.exp(-v))\n",
    "    \n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            v * (1 - v)\n",
    "        \"\"\"\n",
    "        return v * (1 - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationRelu(ActivationFunction):\n",
    "    \"\"\"Relu function.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Init function. Do nothing.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def derivative_use_activated(self):\n",
    "        \"\"\"Use result to calculate derivative.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def apply(self, v):\n",
    "        \"\"\"Apply Relu to data.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): Data\n",
    "        \n",
    "        Returns:\n",
    "            v if v > 0, otherwise 0\n",
    "        \"\"\"\n",
    "        return v.clip(min=0)\n",
    "    \n",
    "    def derivative(self, v):\n",
    "        \"\"\"Calculate derivative wrt input of activation function.\n",
    "        \n",
    "        Args:\n",
    "            v (np.array): input of activation function if derivative_use_activated is false,\n",
    "                output of activation function otherwise\n",
    "        \n",
    "        Returns:\n",
    "            1 if v > 0, otherwise 0\n",
    "        \"\"\"\n",
    "        return np.where(v>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gd updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDUpdater(abc.ABC):\n",
    "    \"\"\"Base class for gradient descent updater. One updater to N layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            weight_infos (dict): buff to store any step result for weight\n",
    "            bias_infos (dict): buff to store any step result for bias terms\n",
    "        \"\"\"\n",
    "        self.weight_infos = {}\n",
    "        self.bias_infos = {}\n",
    "    \n",
    "    def register_layer(self, layer_id, weight_shape, bias_shape):\n",
    "        \"\"\"Register layer which will use this updater.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            weight_shape (int): shape of weight of the layer\n",
    "            bias_shape (int): shape of bias terms of the layer\n",
    "        \"\"\"\n",
    "        self.weight_infos[layer_id] = np.zeros(weight_shape)\n",
    "        self.bias_infos[layer_id] = np.zeros(bias_shape)\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def apply(self, layer_id, grads, bias_grads):\n",
    "        \"\"\"Calculate update value.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weights\n",
    "            bias_grads (np.array): gradients of bias terms\n",
    "        \n",
    "        Returns:\n",
    "            Calculated gradients/update value\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDUpdaterNormal(GDUpdater):\n",
    "    \"\"\"No optimization, apply learning rate only.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            learning_rate (float): learning rate\n",
    "        \"\"\"\n",
    "        GDUpdater.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def apply(self, layer_id, grads, bias_grads):\n",
    "        \"\"\"Calculate update value.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weights\n",
    "            bias_grads (np.array): gradients of bias terms\n",
    "        \n",
    "        Returns:\n",
    "            learning_rate * gradients\n",
    "        \"\"\"\n",
    "        return self.learning_rate * grads, self.learning_rate * bias_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a smaller learning rate(0.01?)\n",
    "class GDUpdaterMomentum(GDUpdater):\n",
    "    \"\"\"Momentum for SGD.\n",
    "    \n",
    "    In Momentum, weight_infos and bias_infos stores the last calculated weight deltas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, gamma=0.9):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            learning_rate (float): learning rate\n",
    "            gamma (float): rate of the update vector of the past time step to add on, default 0.9\n",
    "        \"\"\"\n",
    "        GDUpdater.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def apply(self, layer_id, grads, bias_grads):\n",
    "        \"\"\"Calculate update value.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weights\n",
    "            bias_grads (np.array): gradients of bias terms\n",
    "        \n",
    "        Returns:\n",
    "            gamma * last_gradients + learning_rate * current_gradients\n",
    "        \"\"\"\n",
    "        self.weight_infos[layer_id] = self.gamma*self.weight_infos[layer_id] + self.learning_rate*grads\n",
    "        self.bias_infos[layer_id] = self.gamma*self.bias_infos[layer_id] + self.learning_rate*bias_grads\n",
    "        return self.weight_infos[layer_id], self.bias_infos[layer_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDUpdaterAdagrad(GDUpdater):\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        GDUpdater.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def apply(self, layer_id, grads, bias_grads):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDUpdaterAdadelta(GDUpdater):\n",
    "    def __init__(self, gamma=0.9, epsilon=1e-8):\n",
    "        GDUpdater.__init__(self)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def apply(self, layer_id, grads, bias_grads):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDUpdaterRMSprop(GDUpdater):\n",
    "    \"\"\"RMSprop.\n",
    "    \n",
    "    In RMSprop, weight_infos and bias_infos stores the last calculated E[g^2].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, gamma=0.9, epsilon=1e-8):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            learning_rate (float): learning rate, default 0.001\n",
    "            gamma (float): rate of the update E[g^2] of the past time step to add on, default 0.9\n",
    "            epsilon (float):  smoothing term that avoids division by zero, default 1e-8\n",
    "        \"\"\"\n",
    "        GDUpdater.__init__(self)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, layer_id, grads, bias_grads):\n",
    "        \"\"\"Calculate update value.\n",
    "        \n",
    "        Args:\n",
    "            layer_id (int/str): key to specify the layer\n",
    "            grads (np.array): gradients of weights\n",
    "            bias_grads (np.array): gradients of bias terms\n",
    "        \n",
    "        Returns:\n",
    "            E[g^2] = gamma * last_E[g^2] + (1-gamma) * current_gradients^2\n",
    "            learning_rate * gradients / sqrt(E[g^2] + epsilon)\n",
    "        \"\"\"\n",
    "        self.weight_infos[layer_id] = self.gamma*self.weight_infos[layer_id] \\\n",
    "            + (1-self.gamma)*np.power(grads,2)\n",
    "        self.bias_infos[layer_id] = self.gamma*self.bias_infos[layer_id] \\\n",
    "            + (1-self.gamma)*np.power(bias_grads,2)\n",
    "        weight_delta = self.learning_rate * grads / np.sqrt(self.weight_infos[layer_id] + self.epsilon)\n",
    "        bias_delta = self.learning_rate * bias_grads / np.sqrt(self.bias_infos[layer_id] + self.epsilon)\n",
    "        return weight_delta, bias_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLayer(abc.ABC):\n",
    "    \"\"\"Base class for neural network layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Init function. Nothing needs to do in the base class.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def init(self):\n",
    "        \"\"\"Init function during model creation. Nothing needs to do in the base class.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward(self, step_input, learning=True):\n",
    "        \"\"\"Run through the layer with given input to get output.\n",
    "        \n",
    "        Args:\n",
    "            step_input (np.array): input data\n",
    "            learning (boolean): flag if current step is in learning period\n",
    "                During learning period, intermediate result is stored for back propagation.\n",
    "        \n",
    "        Returns:\n",
    "            Result calculated by current layer\n",
    "            activation(input*weights+bias)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def back_propagation(self, prev_delta, activation_derivatived=False):\n",
    "        \"\"\"Back propagation to calculate gradient in last step for update.\n",
    "        Store the gradients in weight_grads and bias_grads.\n",
    "        \n",
    "        Args:\n",
    "            prev_delta (np.array): delta values(derivatives) from the next layer\n",
    "                wrt output of current layer\n",
    "            activation_derivatived (boolean): if the delta value contains\n",
    "                derivative of activation function\n",
    "        \n",
    "        Returns:\n",
    "            Derivatives of current layer wrt input\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\"Update weights of each input nodes using calculated gradients for last step.\n",
    "        Use pre-specified GD updater for accelerating convergance during GD update.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def xavier_weight_init(self, prev_node_count, cur_node_count, has_bias):\n",
    "        \"\"\"Initialize weight using Xavier init.\n",
    "        \n",
    "        Args:\n",
    "            prev_node_count (int): number of input data (output of previous layer)\n",
    "            cur_node_count (int): number of nodes (output of current layer)\n",
    "            has_bias (boolean): flags if bias terms are added\n",
    "        \n",
    "        Returns:\n",
    "            Initialized weights and bias terms\n",
    "        \"\"\"\n",
    "        weights = np.random.randn(prev_node_count, cur_node_count)/np.sqrt(prev_node_count)\n",
    "        bias = np.random.randn(cur_node_count)/np.sqrt(prev_node_count) \\\n",
    "            if has_bias else np.zeros(cur_node_count)\n",
    "        return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(NNLayer):\n",
    "    \"\"\"Single layer of neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, node_count, has_bias=True, activation=ActivationNone()):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            layer_id (int/str): ID of current layer\n",
    "            node_count (int): number of nodes of current layer\n",
    "            has_bias (boolean): flag if the nodes of the layer has bias terms\n",
    "            activation (ActivationFunction): activation function\n",
    "            gd_updater (GDUpdater): gradient descent updater\n",
    "            weights (np.array): input weights of all nodes\n",
    "            bias (np.array): bias of all nodes\n",
    "            weight_grads (np.array): weight gradients of all nodes of current step \n",
    "            bias_grads (np.array): bias gradients of all nodes of current step \n",
    "            step_input (np.array): input of current step\n",
    "            reduced_sum (np.array): reduced sum of input before activation of current step \n",
    "            step_output (np.array): output of current step\n",
    "        \"\"\"\n",
    "        self.layer_id = None\n",
    "        self.node_count = node_count\n",
    "        self.has_bias = has_bias\n",
    "        self.activation = activation\n",
    "        self.gd_updater = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.weight_grads = None\n",
    "        self.bias_grads = None\n",
    "        self.step_input = None\n",
    "        self.reduced_sum = None\n",
    "        self.step_output = None\n",
    "    \n",
    "    def init(self, prev_node_count, gd_updater):\n",
    "        \"\"\"Init function during model creation. Initialize layer info and nodes.\n",
    "        \n",
    "        Args:\n",
    "            prev_node_count (int): number of input of current layer\n",
    "            gd_updater (GDUpdater): gradient descent updater\n",
    "        \"\"\"\n",
    "        self.layer_id = id(self)\n",
    "        self.gd_updater = gd_updater\n",
    "        self.weights, self.bias = self.xavier_weight_init(prev_node_count, self.node_count, self.has_bias)\n",
    "        self.gd_updater.register_layer(self.layer_id, self.weights.shape, self.bias.shape)\n",
    "    \n",
    "    def forward(self, step_input, learning=True):\n",
    "        \"\"\"Run through the layer with given input to get output.\n",
    "        \n",
    "        Args:\n",
    "            step_input (np.array): input data\n",
    "            learning (boolean): flag if current step is in learning period\n",
    "                During learning period, intermediate result is stored for back propagation.\n",
    "        \n",
    "        Returns:\n",
    "            Result calculated by current layer\n",
    "            activation(input*weights+bias)\n",
    "        \"\"\"\n",
    "        reduced_sum = np.dot(step_input, self.weights) + self.bias\n",
    "        step_output = self.activation.apply(reduced_sum)\n",
    "        if learning:\n",
    "            self.step_input = step_input\n",
    "            self.reduced_sum = reduced_sum\n",
    "            self.step_output = step_output\n",
    "        return step_output\n",
    "    \n",
    "    def back_propagation(self, prev_delta, activation_derivatived=False):\n",
    "        \"\"\"Back propagation to calculate gradient in last step for update.\n",
    "        Store the gradients in weight_grads and bias_grads.\n",
    "        \n",
    "        Args:\n",
    "            prev_delta (np.array): delta values(derivatives) from the next layer\n",
    "                wrt output of current layer\n",
    "            activation_derivatived (boolean): if the delta value contains\n",
    "                derivative of activation function\n",
    "        \n",
    "        Returns:\n",
    "            Derivatives of current layer wrt input\n",
    "        \"\"\"\n",
    "        cur_delta = None\n",
    "        if activation_derivatived:\n",
    "            cur_delta = prev_delta\n",
    "        elif self.activation.derivative_use_activated:\n",
    "            cur_delta = prev_delta * self.activation.derivative(self.step_output)\n",
    "        else:\n",
    "            cur_delta = prev_delta * self.activation.derivative(self.reduced_sum)\n",
    "        self.weight_grads = np.dot(np.atleast_2d(self.step_input).T, cur_delta)/cur_delta.shape[0]\n",
    "        self.bias_grads = cur_delta.mean(axis=0)\n",
    "        return np.dot(cur_delta, self.weights.T)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update weights of each input nodes using calculated gradients for last step.\n",
    "        Use pre-specified GD updater for accelerating convergance during GD update.\n",
    "        \"\"\"\n",
    "        weight_deltas, bias_deltas = \\\n",
    "            self.gd_updater.apply(self.layer_id, self.weight_grads, self.bias_grads)\n",
    "        self.weights -= weight_deltas\n",
    "        if self.has_bias:\n",
    "            self.bias -= bias_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    def __init__(self, X_size, gd_updater):\n",
    "        \"\"\"Init function.\n",
    "        \n",
    "        Attributes:\n",
    "            X_size (int): number of input data\n",
    "            gd_updater (GDUpdater): gradient descent updater for GD update\n",
    "            model (list(NNLayer)): layers of current model\n",
    "        \"\"\"\n",
    "        self.X_size = X_size\n",
    "        self.gd_updater = gd_updater\n",
    "        self.model = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"Add one layer to the model.\n",
    "        \n",
    "        Args:\n",
    "            layer (NNLayer): Layer to be added\n",
    "        \"\"\"\n",
    "        input_size = self.X_size\n",
    "        if self.model:\n",
    "            input_size = self.model[-1].node_count\n",
    "        layer.init(input_size, self.gd_updater)\n",
    "        self.model.append(layer)\n",
    "    \n",
    "    def model_forward(self, X, learning=True):\n",
    "        \"\"\"Forward pass, Run through each layers with given input X to get predict value.\n",
    "        Support both batch and single sample.\n",
    "        Intermediate result is stored for back propagation if learning is set to True.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): input data\n",
    "            learning (boolean): flag if current step is in learning period\n",
    "                During learning period, intermediate result is stored for back propagation.\n",
    "        \n",
    "        Returns:\n",
    "            Result calculated by current model(all layers)\n",
    "        \"\"\"\n",
    "        cur_res = X\n",
    "        for layer in self.model:\n",
    "            cur_res = layer.forward(cur_res, learning=learning)\n",
    "        return cur_res\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Forward pass, run through each layers with given input X to get predict value.\n",
    "        Support both batch and single sample.\n",
    "        Intermediate result is not stored.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): input data\n",
    "        \n",
    "        Returns:\n",
    "            Result calculated by current model\n",
    "        \"\"\"\n",
    "        return self.model_forward(X, learning=False)\n",
    "    \n",
    "    def update_model(self, y_predict, y):\n",
    "        \"\"\"Update parameters of each layer using errs (y_predict - y) in current step.\n",
    "        \n",
    "        Args:\n",
    "            y_predict (np.array): predict values\n",
    "            y (np.array): target data\n",
    "        \"\"\"\n",
    "        delta = np.atleast_2d(y_predict - y)\n",
    "        for layer in reversed(self.model):\n",
    "            delta = layer.back_propagation(delta, activation_derivatived=(layer==self.model[-1]))\n",
    "        for layer in self.model:\n",
    "            layer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sklearn.datasets.load_boston(return_X_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x_org = test_data['data']\n",
    "test_data_y_org = test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_x_org.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_std = (test_data_x_org - np.mean(test_data_x_org, axis=0)) / np.std(test_data_x_org, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = test_data_std[:450]\n",
    "test_data_y = test_data_y_org[:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = NNModel(test_data_x.shape[1], GDUpdaterRMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.add_layer(FCLayer(20, False, ActivationNone()))\n",
    "m.add_layer(FCLayer(node_count=10, activation=ActivationNone()))\n",
    "m.add_layer(FCLayer(node_count=1, activation=ActivationNone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_cost = []\n",
    "all_cost_ce = []\n",
    "for _ in range(rounds):\n",
    "    cost = 0\n",
    "    cost_ce = 0\n",
    "    learning_idx = np.arange(len(test_data_x))\n",
    "    np.random.shuffle(learning_idx)\n",
    "    \n",
    "    for start_idx in range(0, len(test_data_x), batch_size):\n",
    "        data_idx = learning_idx[start_idx : min(start_idx+batch_size,len(test_data_x))]\n",
    "#         data_idx = learning_idx[start_idx]\n",
    "        sample, target = test_data_x[data_idx], test_data_y[data_idx,None]\n",
    "        predict = m.model_forward(sample)\n",
    "        err = predict - target\n",
    "        m.update_model(predict, target)\n",
    "        cost += (err*err).sum()\n",
    "#         cost_ce -= (target*np.log(predict)+(1-target)*np.log(1-predict)).sum()\n",
    "    all_cost.append(cost)\n",
    "    all_cost_ce.append(cost_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10555.44476988625"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cost[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_cost_ce[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xe54dbb0>]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe3cf9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(all_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data_x = test_data_std[450:]\n",
    "predict_data_y = test_data_y_org[450:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = []\n",
    "for sample, tgt in zip(predict_data_x, predict_data_y):\n",
    "    pct = m.model_forward(sample)\n",
    "    predict_y.append(pct)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.accuracy_score(predict_data_y, [1 if pct > 0.5 else 0 for pct in predict_y])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.confusion_matrix(predict_data_y, [1 if pct > 0.5 else 0 for pct in predict_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.360365785711753"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(predict_data_y, predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.4, 15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4,\n",
       "       17.7, 19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6,\n",
       "       23.2, 29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. ,\n",
       "       21.8, 20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8,\n",
       "       24.5, 23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. ,\n",
       "       11.9])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([16.67015233315926]),\n",
       " array([19.980848042032374]),\n",
       " array([19.04222217531037]),\n",
       " array([23.36728988604503]),\n",
       " array([15.363414347558184]),\n",
       " array([15.893719207164278]),\n",
       " array([12.506628768998212]),\n",
       " array([12.700568622780867]),\n",
       " array([17.508780664417216]),\n",
       " array([18.990661286728354]),\n",
       " array([19.537137943199436]),\n",
       " array([20.79259552935968]),\n",
       " array([20.357518627844968]),\n",
       " array([23.322061447320014]),\n",
       " array([20.821352165407163]),\n",
       " array([18.032916353142653]),\n",
       " array([14.340300863001808]),\n",
       " array([17.19222542783421]),\n",
       " array([17.155359141266803]),\n",
       " array([18.793513785587685]),\n",
       " array([20.743208708273162]),\n",
       " array([23.840138991324277]),\n",
       " array([23.14233701414988]),\n",
       " array([26.441944810505447]),\n",
       " array([16.59642958586252]),\n",
       " array([16.224713151116454]),\n",
       " array([21.095798063971685]),\n",
       " array([11.280095125973304]),\n",
       " array([19.65100726033516]),\n",
       " array([22.45744965150684]),\n",
       " array([24.246926890215523]),\n",
       " array([28.26489302941579]),\n",
       " array([29.899032458926758]),\n",
       " array([21.589290891777]),\n",
       " array([19.77753003771975]),\n",
       " array([22.931272256415426]),\n",
       " array([20.258220578320138]),\n",
       " array([21.806350967961553]),\n",
       " array([10.52893630278163]),\n",
       " array([6.5709380009664216]),\n",
       " array([1.5929374463981567]),\n",
       " array([12.652560786952114]),\n",
       " array([14.962335300318408]),\n",
       " array([20.667715481292166]),\n",
       " array([20.559442567008414]),\n",
       " array([16.462870823365346]),\n",
       " array([13.66566642058124]),\n",
       " array([19.232413916740505]),\n",
       " array([21.486407145758452]),\n",
       " array([18.447263048260872]),\n",
       " array([20.706399386618585]),\n",
       " array([24.224497582147084]),\n",
       " array([22.991690896870484]),\n",
       " array([28.802506919416448]),\n",
       " array([27.1961919842311]),\n",
       " array([23.033999668990162])]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
