{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simple_nn.simple_nn as snn\n",
    "import simple_nn.gd_optimizor as gd_opt\n",
    "import simple_nn.activation_function as actv_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common op for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_tgt_values(batch_action,\n",
    "                            batch_q_values, \n",
    "                            batch_next_q_values, \n",
    "                            batch_reward, \n",
    "                            batch_end):\n",
    "    tgt_rewards = batch_reward + discount * np.max(batch_next_q_values, axis=1) * (1-batch_end)\n",
    "    tgt_values = batch_q_values.copy()\n",
    "    tgt_values[np.arange(len(tgt_values)),batch_action] = tgt_rewards\n",
    "    \n",
    "    return tgt_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary expand categorical type to preprocess states\n",
    "def binary_expand(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    res = np.zeros(idx.shape)\n",
    "    res[n==idx] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_expand_mod(n, idx=env.observation_space.n):\n",
    "    if type(idx) is int:\n",
    "        idx = np.array(list(range(idx)))\n",
    "    return np.identity(len(idx))[np.where(n==idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess state information into input parameters\n",
    "preprocess_state = binary_expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning agent - online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 15000\n",
    "learning_rate = 0.02\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.001\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    snn.NNLayer(20, True, actv_func.ActivationNone()),\n",
    "    snn.NNLayer(env.action_space.n, True, actv_func.ActivationNone()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = snn.NNModel(env.observation_space.n, layers, gd_opt.GDOptimizerNone(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 11.364 steps are used, avg total reward is 0.046.\n",
      "In episode 2000, avg 21.996 steps are used, avg total reward is 0.163.\n",
      "In episode 3000, avg 31.066 steps are used, avg total reward is 0.389.\n",
      "In episode 4000, avg 39.125 steps are used, avg total reward is 0.547.\n",
      "In episode 5000, avg 40.672 steps are used, avg total reward is 0.635.\n",
      "In episode 6000, avg 42.074 steps are used, avg total reward is 0.657.\n",
      "In episode 7000, avg 40.993 steps are used, avg total reward is 0.695.\n",
      "In episode 8000, avg 42.601 steps are used, avg total reward is 0.666.\n",
      "In episode 9000, avg 42.897 steps are used, avg total reward is 0.697.\n",
      "In episode 10000, avg 43.027 steps are used, avg total reward is 0.7.\n",
      "In episode 11000, avg 43.731 steps are used, avg total reward is 0.689.\n",
      "In episode 12000, avg 42.778 steps are used, avg total reward is 0.693.\n",
      "In episode 13000, avg 43.658 steps are used, avg total reward is 0.678.\n",
      "In episode 14000, avg 43.855 steps are used, avg total reward is 0.67.\n",
      "In episode 15000, avg 44.027 steps are used, avg total reward is 0.693.\n"
     ]
    }
   ],
   "source": [
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values = m.model_forward(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # calculate error for back propagtion with Bellman equation\n",
    "        tgt_q_values = q_values.copy()\n",
    "        next_q_values = m.predict(new_state)\n",
    "        tgt_q_values[action] = reward + discount * np.max(next_q_values) * (not end)\n",
    "                \n",
    "        # update model with predicted q values\n",
    "        m.update_model(q_values, tgt_q_values)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 3],\n",
       "       [0, 3, 2, 0],\n",
       "       [3, 1, 0, 2],\n",
       "       [1, 2, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = m.predict(np.identity(16))\n",
    "np.argmax(q_values, axis=1).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.7408\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for _ in range(test_episode):\n",
    "    end = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    while not end:\n",
    "        q_values = m.predict(state)\n",
    "        action = np.argmax(q_values)\n",
    "        state_no, reward, end, _ = env.step(action)\n",
    "        state = preprocess_state(state_no)\n",
    "        total_reward += reward\n",
    "print('average reward: {}'.format(total_reward/test_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning agent - experience replay with mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10000\n",
    "learning_rate = 0.1\n",
    "discount = 0.99\n",
    "\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.01\n",
    "epsilon = max_epsilon\n",
    "\n",
    "print_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "max_buf_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    snn.NNLayer(20, True, actv_func.ActivationNone()),\n",
    "    snn.NNLayer(env.action_space.n, True, actv_func.ActivationNone()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = snn.NNModel(env.observation_space.n, layers, gd_opt.GDOptimizerNone(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 1000, avg 34.965 steps are used, avg total reward is 0.515.\n",
      "In episode 2000, avg 43.662 steps are used, avg total reward is 0.69.\n",
      "In episode 3000, avg 44.921 steps are used, avg total reward is 0.66.\n",
      "In episode 4000, avg 43.682 steps are used, avg total reward is 0.7.\n",
      "In episode 5000, avg 44.235 steps are used, avg total reward is 0.692.\n",
      "In episode 6000, avg 42.703 steps are used, avg total reward is 0.704.\n",
      "In episode 7000, avg 43.807 steps are used, avg total reward is 0.656.\n",
      "In episode 8000, avg 44.369 steps are used, avg total reward is 0.659.\n",
      "In episode 9000, avg 42.236 steps are used, avg total reward is 0.691.\n",
      "In episode 10000, avg 43.599 steps are used, avg total reward is 0.679.\n"
     ]
    }
   ],
   "source": [
    "step_count_list = []\n",
    "total_reward_list = []\n",
    "\n",
    "experience_buf = deque(maxlen=max_buf_size)\n",
    "\n",
    "for ep in range(1, episodes+1):\n",
    "    end = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # startup state\n",
    "    new_state = preprocess_state(env.reset())\n",
    "    \n",
    "    # run until game end\n",
    "    while not end:\n",
    "        # predict with the latest model\n",
    "        state = new_state\n",
    "        q_values = m.predict(state)\n",
    "        \n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # step forward\n",
    "        new_state_no, reward, end, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        # update q values with actual returns\n",
    "        # save new state for the next step\n",
    "        new_state = preprocess_state(new_state_no)\n",
    "        \n",
    "        # record to the experience buf\n",
    "        experience_buf.append((state, action, reward, new_state, end))\n",
    "        \n",
    "        # sample and mini batch training\n",
    "        if len(experience_buf) > batch_size:\n",
    "            batch_samples_idx = np.random.choice(np.arange(len(experience_buf)), \n",
    "                                                 size=batch_size, replace=False)\n",
    "            batch_samples = [experience_buf[i] for i in batch_samples_idx]\n",
    "            batch_states = np.array([sample[0] for sample in batch_samples])\n",
    "            batch_action = np.array([sample[1] for sample in batch_samples])\n",
    "            batch_reward = np.array([sample[2] for sample in batch_samples])\n",
    "            batch_new_states = np.array([sample[3] for sample in batch_samples])\n",
    "            batch_end = np.array([sample[4] for sample in batch_samples])\n",
    "            batch_q_values = m.model_forward(batch_states)\n",
    "            batch_next_q_values = m.predict(batch_new_states)\n",
    "\n",
    "            # calculate error for back propagtion with Bellman equation\n",
    "            batch_tgt_q_values = calculate_batch_tgt_values(batch_action,\n",
    "                                                            batch_q_values, \n",
    "                                                            batch_next_q_values, \n",
    "                                                            batch_reward, \n",
    "                                                            batch_end)\n",
    "\n",
    "            # update model with predicted q values\n",
    "            m.update_model(batch_q_values, batch_tgt_q_values)\n",
    "    \n",
    "    # update epsilon\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-epsilon_decay*ep)\n",
    "    \n",
    "    # record step counts\n",
    "    step_count_list.append(step_count)\n",
    "    total_reward_list.append(total_reward)\n",
    "    # print informations\n",
    "    if (ep)%print_step == 0:\n",
    "        print('In episode {}, avg {} steps are used, avg total reward is {}.'.format(\n",
    "            ep, sum(step_count_list)/print_step, sum(total_reward_list)/print_step))\n",
    "        step_count_list.clear()\n",
    "        total_reward_list.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ob model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 3],\n",
       "       [0, 3, 0, 1],\n",
       "       [3, 1, 0, 2],\n",
       "       [3, 2, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = m.predict(np.identity(16))\n",
    "np.argmax(q_values, axis=1).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.7337\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for _ in range(test_episode):\n",
    "    end = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    while not end:\n",
    "        q_values = m.predict(state)\n",
    "        action = np.argmax(q_values)\n",
    "        state_no, reward, end, _ = env.step(action)\n",
    "        state = preprocess_state(state_no)\n",
    "        total_reward += reward\n",
    "print('average reward: {}'.format(total_reward/test_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
